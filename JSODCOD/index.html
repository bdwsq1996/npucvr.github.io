<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Uncertainty-aware Joint Salient Object and Camouflaged Object Detection</title>
  <!-- Bootstrap -->
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<!-- cover -->
<section>
  <div class="jumbotron text-center mt-0">
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2>Uncertainty-aware Joint Salient Object and Camouflaged Object Detection</h2>
          <h4 style="color:#5a6268;">CVPR 2021 (Oral)</h4>
          <hr>
          <h6>
            <a href="http://npu-cvr.cn/" target="_blank">Aixuan Li</a><sup>1*</sup>,
            <a href="http://npu-cvr.cn/" target="_blank">Jing Zhang</a><sup>2<sup>3*</sup>,
            <a href="http://npu-cvr.cn/" target="_blank">Yunqiu Lv</a><sup>1</sup>,
            <a href="http://npu-cvr.cn/" target="_blank">Bowen Liu</a><sup>1</sup>,
            <a href="http://npu-cvr.cn/" target="_blank">Tong Zhang</a><sup>4</sup>,
            <a href="http://npu-cvr.cn/" target="_blank">Yuchao Dai</a><sup>1</sup>
          </h6>
          <p><sup>1</sup>Northwestern Polytechnical University, China &nbsp;&nbsp;
            <sup>2</sup>Australian National University, Australia
            <sup>3</sup>CSIRO, Australia
            <sup>4</sup>EPFL, Switzerland
            <br>
            <sup>*</sup> denotes equal contribution
          </p>


          <div class="row justify-content-center">
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Uncertainty-Aware_Joint_Salient_Object_and_Camouflaged_Object_Detection_CVPR_2021_paper.pdf" role="button"
                  target="_blank">
                  <i class="fa fa-file"></i> Paper</a> </p>
            </div>
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/JingZhang617/Joint_COD_SOD" role="button" target="_blank">
                  <i class="fa fa-github-alt"></i> Code </a> </p>
            </div>
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Li_Uncertainty-Aware_Joint_Salient_CVPR_2021_supplemental.zip" role="button">
                  <i class="fa fa-file"></i> Supplementary </a> </p>
            </div>
            <!-- <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button">
                  <i class="fa fa-database"></i> Data</a> </p>
            </div> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- abstract -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Abstract</h3>
        <hr style="margin-top:0px">
        <p class="text-left"> Visual salient object detection (SOD) aims at finding the salient object(s) that attract human attention, while camouflaged object detection (COD) on the contrary intends to discover the camouflaged object(s) that hidden in the surrounding. In this paper, we propose a paradigm of leveraging the contradictory information to enhance the detection ability of both salient object detection and camouflaged object detection. We start by exploiting the easy positive samples in the COD dataset to serve as hard positive samples in the SOD task to improve the robustness of the SOD model. Then, we introduce a similarity measure module to explicitly model the contradicting attributes of these two tasks.
          Furthermore, considering the uncertainty of labeling in both tasks' datasets, we propose an adversarial learning network to achieve both higher order similarity measure and network confidence estimation. Experimental results on benchmark datasets demonstrate that our solution leads to state-of-the-art (SOTA) performance for both tasks.</p>
      </div>
    </div>
  </div>
</section>
<br>


<!-- Motivation -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Motivation</h3>
        <hr style="margin-top:0px">
        <!-- <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
          <source src="https://www.youtube.com/xxxxx" type="video/mp4">
        </video> -->
        <img class="img-fluid" width="95%" src="image/moti.png" alt="Motivation">
        <p> Illustration of the transition from camouflaged objects to salient objects, where the image in the middle could belong to both camouflaged object dataset and salient object dataset.</p>
      </div>
    </div>
  </div>
</section>
<br>

<!-- showcase -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Network Architecture</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" width="100%" src="image/net.png" alt="Architecture">
        <p> Overview of the proposed network. The Feature encoder module extracts task-specific features for image $X^s$ and $X^c$ from SOD and COD dataset respectively. And for the connection modeling data $X^p$, we introduce Similarity measure to explicitly model the contradicting attribute of SOD and COD.
            The shared Prediction decoder module is used to generate predictions for both tasks.The shared Confidence estimation module is a fully convolutional discriminator, which estimates the pixel-wise confidence of network prediction.  
             </p>
      </div>
    </div>
  </div>
</section>
<br>



<!-- comparison -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Performance comparison with benchmark saliency detection models.</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" width="100%" src="image/sod.png" alt="Performance comparison of SOD.">
      </div>
    </div>
  </div>
</section>
<br>



<!-- comparison -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Predictions of competing salient object detection models and ours.</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" width="100%" src="images/sodimg.png" alt="Performance comparison of SOD.">
      </div>
    </div>
  </div>
</section>
<br>


<!-- comparison -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Performance comparison with re-implemented camouflaged object detection models.</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" width="100%" src="images/cod.png" alt="Performance comparison of COD.">
      </div>
    </div>
  </div>
</section>
<br>



<!-- comparison -->
<section>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center">
        <h3>Predictions of competing camouflaged object detection models and ours.</h3>
        <hr style="margin-top:0px">
        <img class="img-fluid" width="100%" src="images/codimg.png" alt="Performance comparison of COD.">
      </div>
    </div>
  </div>
</section>
<br>



<!-- citing -->
<div class="container">
  <div class="row ">
    <div class="col-12">
      <h3>Citation</h3>
      <hr style="margin-top:0px">
      <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@InProceedings{Li_2021_CVPR,
  author    = {Li, Aixuan and Zhang, Jing and Lv, Yunqiu and Liu, Bowen and Zhang, Tong and Dai, Yuchao},
  title     = {Uncertainty-Aware Joint Salient Object and Camouflaged Object Detection},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2021},
  pages     = {10071-10081}
}
</code></pre>
      <hr>
    </div>
  </div>
</div>

<footer class="text-center" style="margin-bottom:10px">
  <br>
  Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
</footer>

</body>

</html>
